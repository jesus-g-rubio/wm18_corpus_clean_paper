\section{Task description}

Parallel corpus filtering task in WMT18\footnote{\url{http://www.statmt.org/wmt18/parallel-corpus-filtering.html}} tackles the problem of cleaning noisy parallel corpora. Given a noisy parallel corpus (crawled from the web), participants develop methods to filter it to a smaller size of high quality sentence pairs.

Specifically, the organizers provide a very noisy $1$ billion word German--English corpus crawled from the web as part of the Paracrawl project\footnote{\url{https://paracrawl.eu/}}. Participants are asked to select a subset of sentence pairs that amount to (a) $100$ million words, and (b) $10$ million words. The quality of the resulting subsets is determined by the quality of two machine translation systems -one statistical and one neural- trained on the selected data. The quality of the translation systems is measured by BLEU score on the (a) official WMT 2018 news translation test set and (b) another undisclosed test set.

The organizers make explicit that the task addresses the challenge of \emph{data quality} and \emph{not domain-relatedness} of the data for a particular use case. Hence, they discourage participants from sub-sampling the corpus for relevance to the news domain despite being one of the evaluation test sets. Organizers thus place more emphasis on the second undisclosed test set, although they report both scores.

The provided raw parallel corpus is the outcome of a processing pipeline that aimed from high recall at the cost of precision, so it is very noisy. It exhibits noise of all kinds (wrong language in source and target, sentence pairs that are not translations of each other, bad language, incomplete of bad translations, etc.).

Next, we describe how we approached this task.