\section{Corpus filtering as QE task}

We frame the corpus filtering task within the QE framework. Given a pair of sentences $(\mathbf{s}, \mathbf{t})$, we first compute a set of features indicating to what extent the sentences correspond to each other. Then, these features are used to predict a binary score indicating if the sentences in the pair can be considered translations of each other.

In order to make the training process effective, any binary classification model needs to use both positive and negative examples. In our context positive examples are pairs of original and translated sentences, whereas negative examples are sentence pairs that cannot be considered translations of each other. Positive examples can be easily obtained from clean parallel corpora, and, while there is no explicit corpus with negative examples, these can be generated on demand.

We use the confidence score from our binary classifier as the final score for our submission to the shared task. As described in the previous Section, based on this score, the sentence pairs in the original noisy corpus provided by the organizers will be sorted and then the first N pairs will be selected and used to train the MT systems.

Note that this approach may be sub-optimal since it considers each individual pair of sentences in isolation from the rest. In exchange for this, we end up with a much more efficient method, linear in the size of the noisy data.

Next, we describe in detail the features we used for our submission (Sec.~\ref{ssec:features}), the process we followed for generating negative examples (Sec.~\ref{ssec:training}) and the classification model we chose (Sec.~\ref{ssec:model}).

\subsection{Features}
\label{ssec:features}

We use a rich variety of features intended to capture what it means to be an adequate training pair of sentences. For simplicity, we split them into three categories.

\textbf{Adequacy} These features measure how much of the meaning of the original is expressed in the translation and vice versa. We use probabilistic lexicons with different formulations of word alignment to estimate the extent to which the words in the original and translated sentences correspond to each other. 
\begin{itemize}
\item \textit{Average Max lexical probability (2 f.)}: originally proposed by~\cite{Ueffing05} for word-level QE. It measures the average maximum probability of translation for each word in the sentence. We apply it in both source-to-target and target-to-source directions. Formally, source-to-target is given by:
$$ \frac{1}{n}\sum_1^n\max_{j=0}^m P(t_i\mid s_j) $$
where the source word $\mathbf{s}=s_1\ldots s_m$ has $m$ words, the target sentence $\mathbf{t}=t_1\ldots t_n$ has $n$ words and the word $s_0$ indicates the NULL word~\cite{Brown93}. For target-to-source, source and target words swap their roles.
\item \textit{Cross-entropy (2 f.)}: proposed by~\cite{Hainan17}, it measures a ``distance'' between the sentence pairs based on a bag-of-words translation model. Specifically, the ``distance'' is measured as the cross-entropy between the bag-of-words of the actual sentence and the bag-of-words estimated from the other sentence in the pair via the probabilistic lexicon. We apply it in both source-to-target and target-to-source directions.
\end{itemize}

\textbf{Fluency} This type of features aim at capturing if the sentences are well-formed grammatically, contain correct spellings, adhere to common use of terms, titles and names, are intuitively acceptable and can be sensibly interpreted by a native speaker. We use two different features, both based on language models:
\begin{itemize}
\item \textit{Language model score (2 f.)}: given language models for the source and target languages, we use as features the log probability of each sentence in the pair computed with the corresponding model.
\item \textit{Perplexity (2 f.)}: is measured as the inverse probability of the sentence normalized by its number of words. Again, we apply it to both source and target sentences in the pair.
\end{itemize}

\textbf{Shape features} These features can be seen as an extension of adequacy since they measure the mismatch between the frequency of different tokens between the two sentences in the pair; these features are quite commonly used in the QE literature,~\cite{specia15} \textit{inter alia}.
\begin{itemize}
\item \textit{Counts (8 f.)}: count of words, numbers, alphanumeric tokens, and punctuation in both source and target sentences.
\item \textit{Jaccard index (4 f.)}: metric that measures the similarity and diversity of the sets of tokens between the source and target sentences. Formally it is defined as:
$$ \frac{\mid A\cap B\mid}{\mid A\cup B\mid}$$
where $A$ and $B$ are the set of tokens of the source and target sentences respectively. We apply it to words, numbers, alphanumeric tokens and punctuation.
\item \textit{Counts difference (16 f.)}: we compute four metrics from the counts of tokens: the ratio in both directions, the absolute difference, and the absolute difference normalized by the maximum number of tokens of both sentences. Each of these metrics is applied to four different types of tokens: words, numbers, alphanumeric tokens and punctuation.
\item \textit{Specific punctuation (12 f.)} same as the previous features, but in this case we only compute the absolute difference and the normalized difference for specific punctuation tokens: dot (.), comma (,), colon (:), semicolon (;), exclamation mark (!), and question mark (?).
\end{itemize}


\subsection{Training regime}
\label{ssec:training}

An important consideration for this task is how to obtain suitable examples to train the classification model. Positive examples are easy to obtain since any clean parallel corpus provide us with plenty of them. Negative examples, however, are not readily available -there exist no collection of ``wrong'' sentence pairs. Fortunately, they can be easily generated on demand. We mostly followed the approach described in~\cite{Hainan17}, perturbing one or both of the sentences in a pair to create a new synthetic pair that by construction constitutes a negative example.

We apply three different perturbation operations when generating negative pairs:
\begin{itemize}
\item \textit{Swap}: exchange source and target sentences.
\item \textit{Copy}: two copies of the same string. We apply it to both source and target strings.
\item \textit{Randomization}: replace the source or target sentence by a random sentence from the same side of the corpus.
\end{itemize}

As can be seen from above, we focus on the perturbation operations that mess with the correct alignment between the sentences. Thus, we aim at identifying correctly aligned sentence pairs. A complementary approach would be to aim at detecting the actual ``quality'' of the sentence pair, or, in other words, how valuable a sentence pair is when used for training MT systems. However, this is left for future developments.


\subsection{Classification model}
\label{ssec:model}

We did some initial experiments testing the performance of different classifiers on the task of distinguishing between actual original-translation sentence pairs and the synthetically generated negative examples (see Sec.~\ref{ssec:data} for details on the data we used). Gradient boosting algorithm ~\cite{Friedman02} obtained the highest accuracy and, therefore, we used it for our final submission.

Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. Similar to other boosting methods, it builds the models in a stage-wise fashion and it generalizes them by allowing optimization of an arbitrary differentiable loss function\footnote{\url{https://en.wikipedia.org/wiki/Gradient_boosting}}.
