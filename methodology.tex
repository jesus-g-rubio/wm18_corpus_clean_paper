\section{Corpus filtering as QE task}

We frame the corpus filtering task within the QE framework. Given a pair of sentences $(\mathbf{s}, \mathbf{t})$, we first compute a set of features indicating to what extent the sentences correspond to each other. Then, these features are used to predict a binary score indicating if the sentences in the pair can be considered translations of each other.

In order to make the training process effective, any binary classification model needs to use both positive and negative examples. In our context positive examples are pairs of original and translated sentences, whereas negative examples are sentence pairs that cannot be considered translations of each other. Positive examples can be easily obtained from clean parallel corpora, and, while there is no explicit corpus with negative examples, these can be generated on demand.

We use the confidence score from our binary classifier as the final score for our submission to the shared task. As described in the previous Section, based on this score, the sentence pairs in the original noisy corpus provided by the organizers will be sorted and then the first N pairs will be selected and used to train the MT systems.

Note that this approach may be sub-optimal since it considers each individual pair of sentences in isolation from the rest. In exchange for this, we end up with a much more efficient method, linear in the size of the noisy data.

Next, we describe in detail the features we used for our submission (Sec.~\ref{ssec:features}), the classification model we chose (Sec.~\ref{ssec:model}), and the process we followed to train our final submission (Sec.~\ref{ssec:training}).

\subsection{Features}
\label{ssec:features}

We use a rich variety of features intended to capture what it means to be an adequate training pair of sentences. For simplicity, we split them into three categories.

\textbf{Adequacy} These features measure how much of the meaning of the original is expressed in the translation and vice versa. To this end, we use probabilistic lexicons with different formulations on how to align the words of the two sentences to compute how accurate are both sentences translation of each other. 
\begin{itemize}
\item \textit{Average Max lexical probability (2 f.)}: originally proposed by~\cite{Ueffing05} for word-level QE. It measures the average maximum probability of translation from each word in the sentence. We apply it in both source-to-target and target-to-source directions. Formally, it is given by:
$$ \frac{1}{n}\sum_1^n\max_{j=0}^m P(t_i\mid s_j) $$
where the source word $\mathbf{s}=s_1\ldots s_m$ has $m$ words, the target sentence $\mathbf{t}=t_1\ldots t_n$ has $n$ words and the word $s_0$ indicates the NULL word~\cite{Brown93}.
\item \textit{Cross-entropy (2 f.)}: proposed by~\cite{Hainan17}, it measures a ''distance´´ between the sentence pairs based on a bag-of-words translation model. Specifically, the ''distance´´ is measured as the cross-entropy between the bag-of-words of the actual sentence and the bag-of-words estimated from the other sentence in the pair via the probabilistic lexicon. We apply it in both source-to-target and target-to-source directions.
\end{itemize}

\textbf{Fluency} this type of features aim at capturing if the sentences are well-formed grammatically, contain correct spellings, adhere to common use of terms, titles and names, are intuitively acceptable and can be sensibly interpreted by a native speaker. We use two different features, both based on language models:
\begin{itemize}
\item \textit{Language model score (2 f.)}: given language models for the source and target languages, we use af features the score of each sentence in the pair computed with the corresponding model.
\item \textit{Perplexity (2 f.)}: is measured as the inverse probability of the sentence normalized by its number of words. Again, we apply it to both source and target sentences in the pair.
\end{itemize}

\textbf{Shape features} This features can be seen as an extension of adequacy since they measure the mismatch between the frequency of different tokens between the two sentences in the pair; these features are quite commonly used in the QE literature,~\cite{specia15} \textit{inter alia}.
\begin{itemize}
\item \textit{Counts (8 f.)}: count of words, numbers, alphanumeric tokens, and punctuation in both source and target sentences.
\item \textit{Jaccard index (4 f.)}: metric that measures the similarity and diversity of the sets of tokens between the source and target sentences. Formally it is defined as:
$$ \frac{\mid A\cap B\mid}{\mid A\cup B\mid}$$
were $A$ and $B$ are the set of tokens of the source and target sentences respectively. We apply it to words, numbers, alphanumeric tokens and punctuation.
\item \textit{Counts difference (16 f.)}: we compute four metrics from the counts of tokens: the ratio in both directions, the absolute difference, and the absolute difference normalized by the maximum number of tokens of both sentences. Each of this metrics is applied to four different types of tokens: words, numbers, alphanumeric tokens and punctuation.
\item \textit{Specific punctuation (12 f.)} similarly as the previous features, but in this case we only compute the absolute difference and the normalized difference for specific punctuation tokens: dot (.), comma (,), colon (:), semicolon (;), exclamation mark (!), and question mark (?).
\end{itemize}



\subsection{Classification model}
\label{ssec:model}

We did some initial experiments testing different classifiers and gradient boosting~\cite{Friedman02} obtained the best results. Thus, we use gradient boosting as the classification model in our submission.

Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. In particular, we use gradient boosting as implemented in scikit-learn library\footnote{\url{http://scikit-learn.org/stable/index.html}}.


\subsection{Training regime}
\label{ssec:training}

An important consideration for this task is how to obtain suitable examples to train the classification model. Positive examples are easy to obtain since any clean parallel corpus provide us with plenty of them. Negative examples, however, are not readily available -there exist no collection of ''wrong´´ sentence pairs. Fortunately, they can be easily generated on demand. We mostly followed the same approach as~\cite{Hainan17}, perturbing one or both of the sentences in a pair to create a new synthetic pair that by construction constitutes a negative example.

We apply three different perturbation operations when generating negative pairs:
\begin{itemize}
\item \textit{Swap}: exchange source and target sentences.
\item \textit{Copy}: two copies of the same string. We apply it to both source and target strings.
\item \textit{Randomization}: replace the source or target string by a random one.
\end{itemize}

As you can see, we focused on perturbation operations that mess with the correct alignment between sentences. Thus, we aim at identifying correctly aligned sentence pairs. Additionally, we can aim at detecting the actual ''quality´´ of the sentence pair, or, in other words, how valuable is such sentence pair when included in the training data for machine translation. However, this is left for future developments.

